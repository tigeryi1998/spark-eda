# Spark + Kafka EDA Project

This project demonstrates a full workflow for streaming CSV data into Kafka using a Python producer, consuming and analyzing the data using PySpark in Jupyter notebooks, and running services locally using **Podman Compose**. Kubernetes manifests are included but not fully wired yet.

---

## ğŸ“ Project Structure

```text
spark-eda/
â”œâ”€â”€ customers-1000000.csv       # Large CSV dataset used by the producer
â”œâ”€â”€ docker-compose.yml          # Podman/Docker Compose services (Kafka, Producer, EDA)
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ producer/                   # Kafka Producer container
â”‚   â”œâ”€â”€ Containerfile
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ eda/                        # Spark / Jupyter analysis container
â”‚   â”œâ”€â”€ Containerfile
â”‚   â”œâ”€â”€ csv-eda.ipynb           # Local CSV EDA
â”‚   â”œâ”€â”€ kafka-eda.ipynb         # Kafka â†’ Spark decoding + analysis
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ k8s/                        # Kubernetes YAML files (WIP)
    â”œâ”€â”€ buildconfig-producer.yaml
    â”œâ”€â”€ kafka-deployment.yaml
    â”œâ”€â”€ kafka-data-persistentvolumeclaim.yaml
    â”œâ”€â”€ kafka-service.yaml
    â””â”€â”€ producer-deployment.yaml
```

---

## ğŸš€ Running Services with Podman Compose

### **1. Build and start all containers**
```bash
podman-compose up --build -d
```
This starts:
- **Kafka broker**
- **Producer container** (streams CSV â†’ Kafka)
- **EDA container** (Jupyter environment)

---

## ğŸ§ª Testing the Kafka Producer

The producer reads:
```
/app/customers-1000000.csv
```
And sends rows to Kafka topic `customer`.

Check producer logs:
```bash
podman logs -f spark-eda-producer-1
```

---

## ğŸ“Š Running EDA in Jupyter Notebook

The **eda** container exposes JupyterLab.

Find the container:
```bash
podman ps
```
Then inspect Jupyter token:
```bash
podman logs <eda-container-name>
```
Open the printed Jupyter URL in your browser.

### Notebooks:
- **csv-eda.ipynb** â†’ EDA directly on CSV
- **kafka-eda.ipynb** â†’
  - Consume Kafka
  - Decode JSON strings using PySpark UDF
  - Extract columns
  - Run analytics

---

## ğŸ§± Kubernetes (WIP)
YAMLs for Kafka and the producer exist in the `k8s/` folder but are not linked to EDA yet. Future improvements may include:
- Deploying Kafka + Producer in OpenShift
- Adding a Spark deployment
- Using BuildConfig to build images

---

## ğŸ“ Notes
- The project uses **Podman** instead of Docker, but `docker-compose.yml` works with Podman directly.
- Two separate containers are used for Producer and EDA.
- CSV path is mounted into containers via volume.
- Kafka producer runs asynchronously and flushes periodically.

---

## ğŸ”® Future Enhancements
- Kubernetes consumer/EDA deployment
- Add Spark Structured Streaming notebook
- Add schema registry
- Add Airflow or Kafka Connect pipeline

---

## ğŸ¤ Contributing
Feel free to open issues or extend the notebooks.

---

## ğŸ“œ License
Apache 